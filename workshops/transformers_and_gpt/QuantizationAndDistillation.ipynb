{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRldAkpF9e82Fur3vGQFR3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhiga2/AIHawaii/blob/main/workshops/transformers_and_gpt/QuantizationAndDistillation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cheaper, Faster Machine Learning with Quantization and Distillation\n"
      ],
      "metadata": {
        "id": "m8GbsqtM5l-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoawq;\n",
        "!pip install mistralai;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN-EP8hbbmyH",
        "outputId": "528bf58c-9d62-4879-80ee-c0ad8a7c87c4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autoawq in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from autoawq) (2.5.1+cu124)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from autoawq) (3.1.0)\n",
            "Requirement already satisfied: transformers<=4.47.1,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (4.47.1)\n",
            "Requirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (4.12.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from autoawq) (1.3.0)\n",
            "Requirement already satisfied: datasets>=2.20 in /usr/local/lib/python3.11/dist-packages (from autoawq) (3.2.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.23.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.26.5 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.20->autoawq) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.11.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (6.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->autoawq) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (0.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->autoawq) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->autoawq) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.17.0)\n",
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: jsonpath-python>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from mistralai) (1.0.6)\n",
            "Requirement already satisfied: pydantic>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.10.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.8.2)\n",
            "Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->mistralai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->mistralai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.0->mistralai) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.9.0->mistralai) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->mistralai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure that the runtime has a GPU (I chose T4)\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available\")\n",
        "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(\"Device\", i, \":\", torch.cuda.get_device_name(i))\n",
        "else:\n",
        "    print(\"CUDA is not available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSwaleQGOOBJ",
        "outputId": "abe51e78-ac0b-4ad9-96b9-46a641048c9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available\n",
            "Number of GPUs: 1\n",
            "Device 0 : Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization\n",
        "Quantization refers for techniques that reduce the precision of parameters and operations in a deep learning model.\n",
        "\n",
        "## Why quantize?\n",
        "1. Lower memory footprint. Each parameter in the network represented by less bits. GPU memory is expensive!\n",
        "2. Faster inference. Lower precision operations are sometimes faster (highly hardware dependent!).\n",
        "3. Can run on cheaper, less energy-demanding hardware.\n",
        "\n",
        "## What are you trading off?\n",
        "Model will perform a little worse. See [quantized LLM experiments](https://neuralmagic.com/blog/we-ran-over-half-a-million-evaluations-on-quantized-llms-heres-what-we-found/)."
      ],
      "metadata": {
        "id": "Nsp3Vb1J52dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Floating point and fixed point basics\n",
        "* WXAY-(INT or FLOAT): weights quantized to X bits, activations quantized to Y bits. The represetation is either int or float.   \n",
        "* INT = fixed point representation."
      ],
      "metadata": {
        "id": "sK_kZWyAimnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def float_to_fixed(tensor, fractional_bits):\n",
        "    scale = 2 ** fractional_bits\n",
        "    fixed_tensor = (tensor * scale).round().to(torch.int8)\n",
        "\n",
        "    # Ensure the values are within the representable range\n",
        "    min_val = -(2 ** (8 - 1))\n",
        "    max_val = (2 ** (8 - 1)) - 1\n",
        "    fixed_tensor = torch.clamp(fixed_tensor, min_val, max_val)\n",
        "\n",
        "    return fixed_tensor\n",
        "\n",
        "def fixed_to_float(tensor, fractional_bits):\n",
        "     scale = 2 ** fractional_bits\n",
        "     float_tensor = tensor.float() / scale\n",
        "     return float_tensor\n",
        "\n",
        "A = torch.randn(5, 5, dtype=torch.float32)\n",
        "print(A)\n",
        "\n",
        "quantized_A = float_to_fixed(A, 4)\n",
        "print(quantized_A)\n",
        "reconstructed_A = fixed_to_float(quantized_A, 4)\n",
        "print(reconstructed_A)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grS9OLMAjUt_",
        "outputId": "78a20547-4a25-4e0b-fcba-13d80cdce35b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1435, -0.1650, -0.5434, -0.9724, -2.3872],\n",
            "        [ 0.4304, -0.8603, -0.1000, -1.3351, -0.4415],\n",
            "        [-0.6909, -0.8130, -1.6561,  1.8352, -0.1478],\n",
            "        [ 1.0231,  2.2060,  0.5524,  1.0452, -0.0205],\n",
            "        [ 0.9922,  0.3053, -0.1902, -0.6974, -0.9759]])\n",
            "tensor([[  2,  -3,  -9, -16, -38],\n",
            "        [  7, -14,  -2, -21,  -7],\n",
            "        [-11, -13, -26,  29,  -2],\n",
            "        [ 16,  35,   9,  17,   0],\n",
            "        [ 16,   5,  -3, -11, -16]], dtype=torch.int8)\n",
            "tensor([[ 0.1250, -0.1875, -0.5625, -1.0000, -2.3750],\n",
            "        [ 0.4375, -0.8750, -0.1250, -1.3125, -0.4375],\n",
            "        [-0.6875, -0.8125, -1.6250,  1.8125, -0.1250],\n",
            "        [ 1.0000,  2.1875,  0.5625,  1.0625,  0.0000],\n",
            "        [ 1.0000,  0.3125, -0.1875, -0.6875, -1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization Techniques\n",
        "* Quantize aware training: During training, we simulate quantization in the forward pass. This allow the network to adapt to quantization. Both weights and activations are quantized throughout the network before inference (usually results in higher accuracy).\n",
        "* Static post-training quantization: Training occurs in full precision. Weights and activations are quantized before inference.\n",
        "* Dynamic post-training quantization: Weights are quantized before inference, but activations are quantized at runtime.\n",
        "* In this lab, we will focus on a type of post-training quantization: activation-aware weight quantization (AWQ). AWQ preserves 1% of weights at full quantization, while quantizing 99% weights to 4-bits. See autoawq library for quantization: https://docs.vllm.ai/en/latest/features/quantization/auto_awq.html."
      ],
      "metadata": {
        "id": "QD56fa-9iXeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimate memory. These are lower bounds because we are only estimating weights\n",
        "def estimate_memory_gb(num_bits_per_weight: int, num_weights_billions: int):\n",
        "    num_bytes_per_weight = num_bits_per_weight / 8\n",
        "    total_bytes = num_weights_billions * 1e9 * num_bytes_per_weight\n",
        "    total_gb = total_bytes / 2**30\n",
        "    return total_gb\n",
        "\n",
        "def estimate_awq(full_precision_bits: int,\n",
        "                 quantized_bits: int,\n",
        "                 num_weights_billions: int,\n",
        "                 percent_quantized: float):\n",
        "    quantized_mem = estimate_memory_gb(\n",
        "        quantized_bits,\n",
        "        (1 - percent_quantized) * num_weights_billions\n",
        "    )\n",
        "    full_precision_mem = estimate_memory_gb(\n",
        "        quantized_bits,\n",
        "        percent_quantized * num_weights_billions\n",
        "    )\n",
        "    return quantized_mem + full_precision_mem\n",
        "\n",
        "\n",
        "print(\"Estimate memory of mistral-8B: \",\n",
        "      estimate_memory_gb(32, 8), \"GB\")\n",
        "print(\"Estimate memory of mistral-8B_AWQ: \",\n",
        "      estimate_awq(32, 4, 8, 0.01), \"GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBiJJBUteYKM",
        "outputId": "8b112925-46e3-4f6b-b30b-229813476108"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimate memory of mistral-8B:  29.802322387695312 GB\n",
            "Estimate memory of mistral-8B_AWQ:  3.725290298461914 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is the best French cheese?\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "TSGK1UrsgAKY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from mistralai import Mistral\n",
        "api_key = userdata.get('MISTRAL_API_KEY')\n",
        "model = \"ministral-8b-latest\"\n",
        "\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "chat_response = client.chat.complete(\n",
        "    model = model,\n",
        "    messages = messages,\n",
        "    max_tokens = 500,\n",
        ")\n",
        "\n",
        "print(chat_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agrYvk_sRgQ9",
        "outputId": "dffa06dc-1d10-45ec-dc32-3d172c0d4c8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choosing the \"best\" French cheese can be quite subjective as it depends on personal preferences, but here are a few highly regarded French cheeses that are often considered among the best:\n",
            "\n",
            "1. **Roquefort**: This is a sheep's milk cheese with a distinctive blue mold. It has a strong, pungent flavor and is often served with fruit or nuts.\n",
            "\n",
            "2. **Brie de Meaux**: This is a soft, creamy cheese made from cow's milk. It has a mild, buttery flavor and is often served with crackers or bread.\n",
            "\n",
            "3. **Camembert de Normandie**: Another soft cheese, Camembert has a creamy texture and a mild, slightly tangy flavor. It's often served with bread or crackers.\n",
            "\n",
            "4. **Comté**: This is a hard cheese made from cow's milk. It has a nutty, slightly sweet flavor and is often used in cooking.\n",
            "\n",
            "5. **Munster**: This is a soft cheese with a strong, pungent flavor. It's often served with bread or crackers.\n",
            "\n",
            "6. **Reblochon**: This is a soft cheese with a creamy texture and a mild, slightly nutty flavor. It's often served with bread or crackers.\n",
            "\n",
            "7. **Bleu d'Auvergne**: This is a blue cheese made from cow's milk. It has a strong, pungent flavor and is often served with fruit or nuts.\n",
            "\n",
            "Each of these cheeses has its own unique characteristics, so the \"best\" one depends on your personal taste.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a quantized model with AWQ\n",
        "# Had a demo for quantizing comparable model, but free tier colab cannot run it.\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Mistral-7B-v0.1-AWQ\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-v0.1-AWQ\",\n",
        "                                             device_map=\"cuda:0\")\n",
        "\n",
        "tokenized = tokenizer(messages[0][\"content\"], return_tensors=\"pt\").to(\"cuda:0\")\n",
        "output = model.generate(**tokenized, max_new_tokens=200)\n",
        "print(tokenizer.batch_decode(output)[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OMHiREYJml7",
        "outputId": "5aa6b597-8068-4671-a7e9-bbde1ba53aaa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> What is the best French cheese?\n",
            "\n",
            "- The 10 Best French Cheeses\n",
            "- Brie.\n",
            "- Camembert.\n",
            "- Roquefort.\n",
            "- Comté.\n",
            "- Saint-Nectaire.\n",
            "- Cantal.\n",
            "- Brillat-Savarin.\n",
            "\n",
            "## What is the most popular cheese in France?\n",
            "\n",
            "Brie is the most popular cheese in France.\n",
            "\n",
            "## What is the most famous cheese in the world?\n",
            "\n",
            "The 10 Most Popular Cheeses in the World\n",
            "\n",
            "- Brie.\n",
            "- Camembert.\n",
            "- Cheddar.\n",
            "- Feta.\n",
            "- Gouda.\n",
            "- Mozzarella.\n",
            "- Parmesan.\n",
            "- Roquefort.\n",
            "\n",
            "## What is the most famous French cheese?\n",
            "\n",
            "Brie is the most popular cheese in France.\n",
            "\n",
            "## What is the most popular cheese in France 2020?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation\n",
        "Knowledge distillation means having one model (student) learn based on the output of another model (teacher).\n",
        "\n",
        "## Why distill?\n",
        "* Most common usecase is model compression. Often a smaller model can achieve similar performance to a larger model with distillation.\n",
        "* Another common usecase is model simiplification. This aims to make the student model simpler than the teacher model alowing us to run the student model on cheaper hardware.\n",
        "* Mimicing closed source model with open source (may not be legal). [Deepseek is accused of using this technique with chatGPT](https://theconversation.com/openai-says-deepseek-inappropriately-copied-chatgpt-but-its-facing-copyright-claims-too-248863).\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "JqXe7kjGOh26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distillation Techniques\n",
        "* LLMs output distribution of next token in sequence. Train off the distribution of teacher with increased tempurature. In this case, the loss is usually the KL divergence between the student and teacher distillation.\n",
        "* We can also take a hidden layer of the transformer and train based on 1 - cossine similarity.\n",
        "* Both techniques use a glass box method to distillation. Won't be talking too much about block box distillation.   "
      ],
      "metadata": {
        "id": "7Qwm8TIRl_Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vt4Uhvtin6SN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}